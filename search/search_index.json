{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Hi, I'm Richa Kulshrestha \ud83d\udc4b","text":""},{"location":"#senior-data-engineer","title":"Senior Data Engineer","text":"<p>I build scalable data pipelines, optimize warehouse architectures, and turn raw data into reliable insights. Currently working at Booking.com.</p>"},{"location":"#tech-stack","title":"\ud83d\udee0 Tech Stack","text":"<ul> <li>Languages: Python, SQL, PySpark</li> <li>Datalake: Snowflake , Databricks</li> <li>Cloud: AWS (Redshift, Glue, Lambda) </li> <li>Tools: Airflow, Docker, Terraform, DBT</li> </ul> <p>View My Projects{ .md-button .md-button--primary } Contact Me{ .md-button }</p>"},{"location":"projects/etl-pipeline/","title":"AWS Serverless ETL Pipeline","text":""},{"location":"projects/etl-pipeline/#the-problem","title":"The Problem","text":"<p>We needed to ingest 50GB of daily logs from an external API, sanitize PII data, and load it into Redshift for the analytics team, ensuring a latency of under 15 minutes.</p>"},{"location":"projects/etl-pipeline/#the-architecture","title":"The Architecture","text":"<p>```mermaid graph LR     A[API Source] --&gt;|JSON| B(AWS Lambda);     B --&gt;|Raw Data| C[(S3 Bucket)];     C --&gt;|Trigger| D{AWS Glue};     D --&gt;|Cleaned Parquet| E[(Redshift)];     D --&gt;|Logs| F[CloudWatch];</p> <ol> <li>Ingestion: AWS Lambda fetches data from API every 10 mins.</li> <li>Storage: Raw JSON lands in S3 (Bronze Layer).</li> <li>Processing: Glue Job converts JSON to Parquet and handles PII masking.</li> <li>Warehousing: Data is loaded into Redshift (Silver Layer).</li> </ol>"},{"location":"projects/etl-pipeline/#key-code-snippet","title":"Key Code Snippet","text":"<p>Here is how I optimized the Lambda memory usage:</p> <p>```python def lambda_handler(event, context):     # Streaming response to keep memory footprint low     with requests.get(url, stream=True) as r:         upload_to_s3(r.raw)</p>"}]}